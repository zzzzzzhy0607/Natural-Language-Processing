{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANLY580 Project 1\n",
    "\n",
    "### 404 Not Found -- Heng Zhou, Hongyang Zheng, Zhengqian Xu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hongyang_zheng/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/hongyang_zheng/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.util import ngrams \n",
    "from collections import Counter\n",
    "import string\n",
    "from statistics import stdev\n",
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords')\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from prettytable import PrettyTable\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Message Polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data without Emoji\n",
    "def read(dataset):\n",
    "    \n",
    "    with open('./data/Gold/'+dataset, encoding=\"utf-8\") as file:\n",
    "        train=[]\n",
    "        train_y = []\n",
    "        for line in file:\n",
    "            tweet_train=line.split('\\t')[2] \n",
    "            tweet_train=tweet_train.encode('ascii', 'ignore').decode('ascii')\n",
    "            train.append(tweet_train[:-1])\n",
    "            train_y.append(line.split('\\t')[1])\n",
    "    return train, train_y\n",
    "\n",
    "train, train_y = read('train.txt')\n",
    "dev, dev_y = read('dev.txt')\n",
    "devt, devt_y = read('devtest.txt')\n",
    "test, test_y = read('test.txt')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will write functions to preprocess the data in the following aspects:\n",
    "* remove URL\n",
    "* remove stopwords\n",
    "* remove Profiles\n",
    "* remove Hashtags\n",
    "* convert to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing functions\n",
    "def process_tweets(tweets):\n",
    "    \n",
    "    tweets_hash=[]\n",
    "    tweets_punc=[]\n",
    "    tweets_clean=[]\n",
    "\n",
    "    for tweet in tweets:\n",
    "          \n",
    "        # Process text\n",
    "        tokens = nltk.casual_tokenize(tweet)\n",
    "\n",
    "        # Remove URL and stopwords\n",
    "        tokens =[term for term in tokens if not term.startswith('https://') ]\n",
    "        tokens =[term for term in tokens if not term.startswith('http://') ]\n",
    "        \n",
    "        # Remove stopwords\n",
    "        stop_words = list(set(stopwords.words('english')))\n",
    "        tokens = [term for term in tokens if not term in stop_words]\n",
    "\n",
    "        # Remove profiles\n",
    "        tokens1 = [term for term in tokens if not term.startswith('@')]\n",
    "        \n",
    "        # convert to lower case\n",
    "        words = [w.lower() for w in tokens1]\n",
    "        \n",
    "        # tweets with hashtag and punctuation\n",
    "        sentence1 = ' '.join(words)\n",
    "        tweets_hash.append(sentence1)\n",
    "\n",
    "        # Remove hashtags\n",
    "        tokens2 = [term for term in words if not term.startswith('#')]\n",
    "        \n",
    "        # tweets with punctuation\n",
    "        sentence2 = ' '.join(tokens2)\n",
    "        tweets_punc.append(sentence2)\n",
    "        \n",
    "        # remove punctuations\n",
    "        punctuation = list(string.punctuation)\n",
    "        clean = [term for term in tokens2 if term not in punctuation]\n",
    "        \n",
    "        # clean tweets\n",
    "        sentence3 = ' '.join(clean)\n",
    "        tweets_clean.append(sentence3)\n",
    "         \n",
    "    return tweets_hash, tweets_clean, tweets_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create different datasets\n",
    "train_hash, train_clean, train_punc = process_tweets(train)\n",
    "\n",
    "dev_hash, dev_clean, dev_punc = process_tweets(dev)\n",
    "\n",
    "devt_hash, devt_clean, devt_punc = process_tweets(devt)\n",
    "\n",
    "test_hash, test_clean, test_punc = process_tweets(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will try some stemming/lemmatization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(data):\n",
    "    lemmatiser = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    sentence_stem = []\n",
    "    \n",
    "    for tweet in data:\n",
    "        \n",
    "        sentence = tweet.split()  \n",
    "        stem = [stemmer.stem(word) for word in sentence]\n",
    "        sentence_stem.append(\" \".join(stem))\n",
    "    \n",
    "    return sentence_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(X_train, X_test):\n",
    "    \n",
    "    # Initialize vectorizer\n",
    "    vectorizer = CountVectorizer(lowercase=False)\n",
    "    # Fit\n",
    "    vectorizer.fit(X_train)\n",
    "    \n",
    "    # Transform X_train and X_test\n",
    "    X_train_dtm = vectorizer.transform(X_train)\n",
    "    X_test_dtm = vectorizer.transform(X_test)\n",
    "    \n",
    "    return X_train_dtm, X_test_dtm\n",
    "\n",
    "# Convert label to a numerical variable\n",
    "def convert_label(x, y):\n",
    "    \n",
    "    # Convert to dataframe\n",
    "    df_dict = {'text':x,'label':y}\n",
    "    df = pd.DataFrame(df_dict)\n",
    "    \n",
    "    # Delete NA\n",
    "    df=df[df['text']!= \"\"]\n",
    "    \n",
    "    # Label\n",
    "    df['label'] = df['label'].map({'positive':0, 'neutral':1, 'negative':2})\n",
    "    X = df['text']\n",
    "    Y = df['label']\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexicon-based Sentiment Analyzers -- Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Vader and store evaluation score\n",
    "def Vader(tweets, Y_test):\n",
    "    \n",
    "    Y_prediction=[]\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        vs = analyzer.polarity_scores(tweet)\n",
    "        if vs['compound'] >= 0.05 : \n",
    "            Y_prediction.append(\"positive\")\n",
    "        elif vs['compound'] <= - 0.05 : \n",
    "            Y_prediction.append(\"negative\") \n",
    "        else: \n",
    "            Y_prediction.append(\"neutral\")\n",
    "            \n",
    "    target_names = ['positive', 'neutral', 'negative']\n",
    "    print('Classification Report for Vader:')\n",
    "    print(classification_report(y_true=Y_test, y_pred=Y_prediction, target_names=target_names))\n",
    "    \n",
    "    precision,recall,fscore=score(Y_test, Y_prediction)[:3]\n",
    "    res=[]\n",
    "    res.append('Vader')\n",
    "    for x in fscore:\n",
    "        res.append(round(x,2))\n",
    "    res.append(round(np.mean(fscore),2))\n",
    " \n",
    "    for x in recall:\n",
    "        res.append(round(x,2))\n",
    "    res.append(round(np.mean(recall),2))\n",
    "\n",
    "    for x in precision:\n",
    "        res.append(round(x,2))\n",
    "    res.append(round(np.mean(precision),2))\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Naive Bayes and store evaluation score\n",
    "def Naive_bayes(X_train, Y_train, X_test, Y_test,a, if_fit=True,class_prob = None):\n",
    "    \n",
    "    nb = MultinomialNB(alpha=a, fit_prior=if_fit, class_prior= class_prob)\n",
    "    nb.fit(X_train, Y_train)\n",
    "    Y_prediction = nb.predict(X_test)\n",
    "    \n",
    "    target_names = ['positive', 'neutral', 'negative']\n",
    "    print('Classification Report for Naive Bayes:')\n",
    "    print(classification_report(y_true=Y_test, y_pred=Y_prediction, target_names=target_names))\n",
    "\n",
    "    precision,recall,fscore = score(Y_test, Y_prediction)[:3]\n",
    "    \n",
    "    res=[]\n",
    "    res.append('Naive Bayes')\n",
    "    for x in fscore:\n",
    "        res.append(round(x,2))\n",
    "    res.append(round(np.mean(fscore),2))\n",
    " \n",
    "    for x in recall:\n",
    "        res.append(round(x,2))\n",
    "    res.append(round(np.mean(recall),2))\n",
    "\n",
    "    for x in precision:\n",
    "        res.append(round(x,2))\n",
    "    res.append(round(np.mean(precision),2))\n",
    "    \n",
    "    return res      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for logistic and store evaluation score\n",
    "def Logistic(X_train_dtm, Y_train, X_test_dtm, Y_test, c, n):\n",
    "    \n",
    "    logreg = LogisticRegression(C=c, max_iter =n, class_weight='balanced', solver='newton-cg')\n",
    "    logreg.fit(X_train_dtm, Y_train)\n",
    "    Y_prediction = logreg.predict(X_test_dtm)\n",
    "    \n",
    "    target_names = ['positive', 'neutral', 'negative']\n",
    "    print('Classification Report for Logistic Regression:')\n",
    "    print(classification_report(y_true=Y_test, y_pred=Y_prediction, target_names=target_names))\n",
    "    \n",
    "    precision,recall,fscore=score(Y_test, Y_prediction)[:3]\n",
    "    \n",
    "    res=[]\n",
    "    res.append('Logistic')\n",
    "    for x in fscore:\n",
    "        res.append(round(x,2))\n",
    "    res.append(round(np.mean(fscore),2))\n",
    " \n",
    "    for x in recall:\n",
    "        res.append(round(x,2))\n",
    "    res.append(round(np.mean(recall),2))\n",
    "\n",
    "    for x in precision:\n",
    "        res.append(round(x,2))\n",
    "    res.append(round(np.mean(precision),2))\n",
    "    \n",
    "    return res  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print performance table\n",
    "def performance(A1,A2,A3):\n",
    "    t = PrettyTable(['Classifier', 'F1+', 'F1|', 'F1-', 'F1_avg', 'R+', 'R|', 'R-', 'R_avg', \\\n",
    "                   'P+', 'P|', 'P-', 'P_avg'])\n",
    "    t.add_row(A1)\n",
    "    t.add_row(A2)\n",
    "    t.add_row(A3) \n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting experiments, we tried different combinations of datasets as training data and compared the relative  performance. Finally we decided to use `train+dev+test` as training data and `devt` as test data.\n",
    "\n",
    "Also, we tried different parameters for naive bayes and logistic regression and use the best model for below experiments. \n",
    "\n",
    "Below Experiments are based on different preprocessing and with/without stemming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1 -- Use Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Vader:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive       0.37      0.57      0.45      4485\n",
      "    neutral       0.60      0.36      0.45     13150\n",
      "   negative       0.52      0.66      0.58     10996\n",
      "\n",
      "avg / total       0.53      0.51      0.50     28631\n",
      "\n",
      "Classification Report for Naive Bayes:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive       0.69      0.57      0.62       994\n",
      "    neutral       0.41      0.46      0.43       681\n",
      "   negative       0.40      0.53      0.45       325\n",
      "\n",
      "avg / total       0.55      0.52      0.53      2000\n",
      "\n",
      "Classification Report for Logistic Regression:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive       0.70      0.60      0.65       994\n",
      "    neutral       0.45      0.51      0.48       681\n",
      "   negative       0.42      0.49      0.45       325\n",
      "\n",
      "avg / total       0.57      0.55      0.56      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model 1\n",
    "model_x = train_clean+dev_clean+test_clean\n",
    "model_y = train_y+ dev_y+test_y\n",
    "\n",
    "x_test = devt_clean[:]\n",
    "y_test = devt_y[:]\n",
    "X_train, Y_train=convert_label(model_x, model_y)\n",
    "X_test, Y_test=convert_label(x_test, y_test)\n",
    "X_train_dtm, X_test_dtm=vectorizer(X_train, X_test)\n",
    "\n",
    "Vader_Score1=Vader(model_x, model_y)\n",
    "Naive_Bayes_Score1=Naive_bayes(X_train_dtm, Y_train, X_test_dtm, Y_test, a=1.0, if_fit=True, class_prob = [0.2,0.3,0.5])\n",
    "Log_Score1=Logistic(X_train_dtm, Y_train, X_test_dtm, Y_test, c=0.1, n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+------+------+--------+------+------+------+-------+------+------+------+-------+\n",
      "|  Classifier | F1+  | F1|  | F1-  | F1_avg |  R+  |  R|  |  R-  | R_avg |  P+  |  P|  |  P-  | P_avg |\n",
      "+-------------+------+------+------+--------+------+------+------+-------+------+------+------+-------+\n",
      "|    Vader    | 0.45 | 0.45 | 0.58 |  0.5   | 0.57 | 0.36 | 0.66 |  0.53 | 0.37 | 0.6  | 0.52 |  0.5  |\n",
      "| Naive Bayes | 0.62 | 0.43 | 0.45 |  0.5   | 0.57 | 0.46 | 0.53 |  0.52 | 0.69 | 0.41 | 0.4  |  0.5  |\n",
      "|   Logistic  | 0.65 | 0.48 | 0.45 |  0.53  | 0.6  | 0.51 | 0.49 |  0.54 | 0.7  | 0.45 | 0.42 |  0.53 |\n",
      "+-------------+------+------+------+--------+------+------+------+-------+------+------+------+-------+\n"
     ]
    }
   ],
   "source": [
    "performance(Vader_Score1, Naive_Bayes_Score1, Log_Score1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2 -- Use Data with Hashtag and Punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Vader:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive       0.37      0.57      0.45      4485\n",
      "    neutral       0.60      0.36      0.45     13150\n",
      "   negative       0.52      0.66      0.58     10996\n",
      "\n",
      "avg / total       0.53      0.51      0.50     28631\n",
      "\n",
      "Classification Report for Naive Bayes:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive       0.69      0.56      0.62       994\n",
      "    neutral       0.41      0.45      0.43       681\n",
      "   negative       0.39      0.51      0.44       325\n",
      "\n",
      "avg / total       0.54      0.52      0.53      2000\n",
      "\n",
      "Classification Report for Logistic Regression:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive       0.70      0.61      0.65       994\n",
      "    neutral       0.46      0.51      0.48       681\n",
      "   negative       0.43      0.49      0.46       325\n",
      "\n",
      "avg / total       0.57      0.56      0.56      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model 2\n",
    "model_x = train_hash+dev_hash+test_hash\n",
    "model_y = train_y+ dev_y+test_y\n",
    "\n",
    "x_test = devt_hash[:]\n",
    "y_test = devt_y[:]\n",
    "\n",
    "X_train, Y_train=convert_label(model_x, model_y)\n",
    "X_test, Y_test=convert_label(x_test, y_test)\n",
    "X_train_dtm, X_test_dtm=vectorizer(X_train, X_test)\n",
    "\n",
    "Vader_Score2=Vader(model_x, model_y)\n",
    "Naive_Bayes_Score2=Naive_bayes(X_train_dtm, Y_train, X_test_dtm, Y_test, a=1.0, if_fit=True, class_prob = [0.2,0.3,0.5])\n",
    "Log_Score2=Logistic(X_train_dtm, Y_train, X_test_dtm, Y_test, c=0.2, n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+------+------+--------+------+------+------+-------+------+------+------+-------+\n",
      "|  Classifier | F1+  | F1|  | F1-  | F1_avg |  R+  |  R|  |  R-  | R_avg |  P+  |  P|  |  P-  | P_avg |\n",
      "+-------------+------+------+------+--------+------+------+------+-------+------+------+------+-------+\n",
      "|    Vader    | 0.45 | 0.45 | 0.58 |  0.5   | 0.57 | 0.36 | 0.66 |  0.53 | 0.37 | 0.6  | 0.52 |  0.5  |\n",
      "| Naive Bayes | 0.62 | 0.43 | 0.44 |  0.5   | 0.56 | 0.45 | 0.51 |  0.51 | 0.69 | 0.41 | 0.39 |  0.49 |\n",
      "|   Logistic  | 0.65 | 0.48 | 0.46 |  0.53  | 0.61 | 0.51 | 0.49 |  0.54 | 0.7  | 0.46 | 0.43 |  0.53 |\n",
      "+-------------+------+------+------+--------+------+------+------+-------+------+------+------+-------+\n"
     ]
    }
   ],
   "source": [
    "performance(Vader_Score2, Naive_Bayes_Score2, Log_Score2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above result, we can see that compared with clean data, data with hashtag and punctuation does not improve the model performance. Actually, the performance of naive bayes model decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3 -- Use Data with Punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Vader:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive       0.37      0.57      0.45      4485\n",
      "    neutral       0.60      0.36      0.45     13150\n",
      "   negative       0.52      0.66      0.58     10996\n",
      "\n",
      "avg / total       0.53      0.51      0.50     28631\n",
      "\n",
      "Classification Report for Naive Bayes:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive       0.69      0.57      0.62       994\n",
      "    neutral       0.41      0.46      0.43       681\n",
      "   negative       0.40      0.53      0.45       325\n",
      "\n",
      "avg / total       0.55      0.52      0.53      2000\n",
      "\n",
      "Classification Report for Logistic Regression:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive       0.70      0.60      0.65       994\n",
      "    neutral       0.45      0.51      0.48       681\n",
      "   negative       0.42      0.49      0.45       325\n",
      "\n",
      "avg / total       0.57      0.55      0.56      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model 3\n",
    "model_x = train_punc+dev_punc+test_punc\n",
    "model_y = train_y+ dev_y+test_y\n",
    "\n",
    "x_test = devt_punc[:]\n",
    "y_test = devt_y[:]\n",
    "\n",
    "X_train, Y_train=convert_label(model_x, model_y)\n",
    "X_test, Y_test=convert_label(x_test, y_test)\n",
    "X_train_dtm, X_test_dtm=vectorizer(X_train, X_test)\n",
    "\n",
    "Vader_Score3=Vader(model_x, model_y)\n",
    "Naive_Bayes_Score3=Naive_bayes(X_train_dtm, Y_train, X_test_dtm, Y_test, a=1.0, if_fit=True, class_prob = [0.2,0.3,0.5])\n",
    "Log_Score3=Logistic(X_train_dtm, Y_train, X_test_dtm, Y_test, c=0.1, n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+------+------+--------+------+------+------+-------+------+------+------+-------+\n",
      "|  Classifier | F1+  | F1|  | F1-  | F1_avg |  R+  |  R|  |  R-  | R_avg |  P+  |  P|  |  P-  | P_avg |\n",
      "+-------------+------+------+------+--------+------+------+------+-------+------+------+------+-------+\n",
      "|    Vader    | 0.45 | 0.45 | 0.58 |  0.5   | 0.57 | 0.36 | 0.66 |  0.53 | 0.37 | 0.6  | 0.52 |  0.5  |\n",
      "| Naive Bayes | 0.62 | 0.43 | 0.45 |  0.5   | 0.57 | 0.46 | 0.53 |  0.52 | 0.69 | 0.41 | 0.4  |  0.5  |\n",
      "|   Logistic  | 0.65 | 0.48 | 0.45 |  0.53  | 0.6  | 0.51 | 0.49 |  0.54 | 0.7  | 0.45 | 0.42 |  0.53 |\n",
      "+-------------+------+------+------+--------+------+------+------+-------+------+------+------+-------+\n"
     ]
    }
   ],
   "source": [
    "performance(Vader_Score3, Naive_Bayes_Score3, Log_Score3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data only with punctuation seems as good as the clean data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4 -- Use Data with Punctuation and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stemmed datasets - data with punctuation\n",
    "train_stem = stem(train_punc)\n",
    "dev_stem = stem(dev_punc)\n",
    "devt_stem = stem(devt_punc)\n",
    "test_stem = stem(test_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Vader:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive       0.38      0.53      0.44      4485\n",
      "    neutral       0.56      0.42      0.48     13150\n",
      "   negative       0.51      0.58      0.55     10996\n",
      "\n",
      "avg / total       0.51      0.50      0.50     28631\n",
      "\n",
      "Classification Report for Naive Bayes:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive       0.70      0.55      0.62       994\n",
      "    neutral       0.41      0.42      0.41       681\n",
      "   negative       0.37      0.58      0.45       325\n",
      "\n",
      "avg / total       0.55      0.51      0.52      2000\n",
      "\n",
      "Classification Report for Logistic Regression:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive       0.70      0.62      0.66       994\n",
      "    neutral       0.46      0.48      0.47       681\n",
      "   negative       0.40      0.51      0.45       325\n",
      "\n",
      "avg / total       0.57      0.55      0.56      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model 4\n",
    "model_x = train_stem+dev_stem+test_stem\n",
    "model_y = train_y+ dev_y+test_y\n",
    "\n",
    "x_test = devt_stem[:]\n",
    "y_test = devt_y[:]\n",
    "\n",
    "X_train, Y_train=convert_label(model_x, model_y)\n",
    "X_test, Y_test=convert_label(x_test, y_test)\n",
    "X_train_dtm, X_test_dtm=vectorizer(X_train, X_test)\n",
    "\n",
    "Vader_Score4=Vader(model_x, model_y)\n",
    "Naive_Bayes_Score4=Naive_bayes(X_train_dtm, Y_train, X_test_dtm, Y_test, a=1.0, if_fit=True, class_prob = [0.2,0.3,0.5])\n",
    "Log_Score4=Logistic(X_train_dtm, Y_train, X_test_dtm, Y_test, c=0.5, n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+------+------+--------+------+------+------+-------+------+------+------+-------+\n",
      "|  Classifier | F1+  | F1|  | F1-  | F1_avg |  R+  |  R|  |  R-  | R_avg |  P+  |  P|  |  P-  | P_avg |\n",
      "+-------------+------+------+------+--------+------+------+------+-------+------+------+------+-------+\n",
      "|    Vader    | 0.44 | 0.48 | 0.55 |  0.49  | 0.53 | 0.42 | 0.58 |  0.51 | 0.38 | 0.56 | 0.51 |  0.48 |\n",
      "| Naive Bayes | 0.62 | 0.41 | 0.45 |  0.49  | 0.55 | 0.42 | 0.58 |  0.52 | 0.7  | 0.41 | 0.37 |  0.49 |\n",
      "|   Logistic  | 0.66 | 0.47 | 0.45 |  0.53  | 0.62 | 0.48 | 0.51 |  0.54 | 0.7  | 0.46 | 0.4  |  0.52 |\n",
      "+-------------+------+------+------+--------+------+------+------+-------+------+------+------+-------+\n"
     ]
    }
   ],
   "source": [
    "performance(Vader_Score4, Naive_Bayes_Score4, Log_Score4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of this kind of combination of preprocessing is not good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 5 -- Use Clean Data with Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stemmed datasets - clean data\n",
    "train_stem = stem(train_clean)\n",
    "dev_stem = stem(dev_clean)\n",
    "devt_stem = stem(devt_clean)\n",
    "test_stem = stem(test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Vader:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive       0.38      0.53      0.44      4485\n",
      "    neutral       0.56      0.42      0.48     13150\n",
      "   negative       0.51      0.58      0.55     10996\n",
      "\n",
      "avg / total       0.51      0.50      0.50     28631\n",
      "\n",
      "Classification Report for Naive Bayes:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive       0.70      0.55      0.62       994\n",
      "    neutral       0.41      0.42      0.41       681\n",
      "   negative       0.37      0.58      0.45       325\n",
      "\n",
      "avg / total       0.55      0.51      0.52      2000\n",
      "\n",
      "Classification Report for Logistic Regression:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive       0.70      0.61      0.65       994\n",
      "    neutral       0.45      0.49      0.47       681\n",
      "   negative       0.40      0.50      0.45       325\n",
      "\n",
      "avg / total       0.57      0.55      0.56      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_x = train_stem+dev_stem+test_stem\n",
    "model_y = train_y+ dev_y+test_y\n",
    "\n",
    "x_test = devt_stem[:]\n",
    "y_test = devt_y[:]\n",
    "\n",
    "X_train, Y_train=convert_label(model_x, model_y)\n",
    "X_test, Y_test=convert_label(x_test, y_test)\n",
    "X_train_dtm, X_test_dtm=vectorizer(X_train, X_test)\n",
    "\n",
    "Vader_Score5=Vader(model_x, model_y)\n",
    "Naive_Bayes_Score5=Naive_bayes(X_train_dtm, Y_train, X_test_dtm, Y_test, a=1.0, if_fit=True, class_prob = [0.2,0.3,0.5])\n",
    "Log_Score5=Logistic(X_train_dtm, Y_train, X_test_dtm, Y_test, c=0.1, n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+------+------+--------+------+------+------+-------+------+------+------+-------+\n",
      "|  Classifier | F1+  | F1|  | F1-  | F1_avg |  R+  |  R|  |  R-  | R_avg |  P+  |  P|  |  P-  | P_avg |\n",
      "+-------------+------+------+------+--------+------+------+------+-------+------+------+------+-------+\n",
      "|    Vader    | 0.44 | 0.48 | 0.55 |  0.49  | 0.53 | 0.42 | 0.58 |  0.51 | 0.38 | 0.56 | 0.51 |  0.48 |\n",
      "| Naive Bayes | 0.62 | 0.41 | 0.45 |  0.49  | 0.55 | 0.42 | 0.58 |  0.52 | 0.7  | 0.41 | 0.37 |  0.49 |\n",
      "|   Logistic  | 0.65 | 0.47 | 0.45 |  0.52  | 0.61 | 0.49 | 0.5  |  0.53 | 0.7  | 0.45 | 0.4  |  0.52 |\n",
      "+-------------+------+------+------+--------+------+------+------+-------+------+------+------+-------+\n"
     ]
    }
   ],
   "source": [
    "performance(Vader_Score5, Naive_Bayes_Score5, Log_Score5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also not a good choice. So we finally choose to use the preprocessing method for clean data to preprocess the input data. For the model part, logistic model seems always outperform the other two methods regardless how we preprocess the dataset. So we will use logistic model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 Speculate on the differences between the two performance measures above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "Recall = \\frac{True Positive}{Actual Positive} =  \\frac{True Positive}{True Positive+False Negative}  \\\\\n",
    "Precision = \\frac{True Positive}{Predict Positive} =  \\frac{True Positive}{True Positive+False Positive}  \\\\\n",
    "F1 score = 2*\\frac{Precision*Recall}{Precision+Recall}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall refers to the fraction of relevant instances that have been retrieved over the total amount of relevant instances. For example, the $R_+$ means fraction of positive tweets we have captured among all actual positive tweets.   \n",
    "\n",
    "Precision refers to the fraction of relevant instances among the retrieved instances. For example, the $P_+$ means fraction of positive tweets we have captured among all predicted positive tweets.  \n",
    "\n",
    " F1-Score is the weighted average of Precision and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 Look at your results and find / show examples where your classifiers have mis-performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_x = train_clean+dev_clean+test_clean\n",
    "model_y = train_y+ dev_y+test_y\n",
    "\n",
    "x_test = devt_clean[:]\n",
    "y_test = devt_y[:]\n",
    "X_train, Y_train=convert_label(model_x, model_y)\n",
    "X_test, Y_test=convert_label(x_test, y_test)\n",
    "X_train_dtm, X_test_dtm=vectorizer(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='newton-cg', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(C=0.1, max_iter =100, class_weight='balanced', solver='newton-cg')\n",
    "logreg.fit(X_train_dtm, Y_train)\n",
    "Y_prediction = logreg.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame(columns=['text','label','prediction'])\n",
    "df_result['text'] = x_test\n",
    "df_result['label'] = Y_test\n",
    "df_result['prediction'] = Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>everybody chill sony may separate global flags...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>sony announced new 500 gb playstation 4 bundle...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>could hit 3rd console war wrote characters wen...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>sun daily lenovo sony marshall london on-trend...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>one voted 5th popular song time poll conducted...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>that's i think may weird last minute deal sony...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>sony 1st 1 film summer war room fought compton...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>uncharted march may thing bring back sony</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>ok ... sony announced playable games line toky...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>i got love sony they disappear tomorrow someon...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>sony pictures last monday sony pictures releas...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>sony's project morpheus review a virtual reali...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>for info would need contact sony apologies inc...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>tomorrow morning usa vs brazil live sony six h...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>sony xperia z5 premium news sony may found sol...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>it deepika tamasha :) sony ... may sony one br...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>sony's 20th anniversary sale going</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>ms using strategy sony used last gen spend 1st...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>ps4 firmware 3.0 as may know sony releasing 3....</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>sony zeiss showdown announcement friday 11 cam...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>sony's 20th anniversary classic playstation :d</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>apple microsoft 1st pairing tech bastards cine...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>here's gets wrong star wars toys right force f...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>um ... i know familiar star wars i shops satur...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>if married someone like star wars may divorce</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>everyone definitely go watch star wars decembe...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>did disney make back 3/4 4 billion paid lucasf...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>why never old star wars may force</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>in mood watch star wars like 2 i class tomorrow</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>i've problems email received information satur...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label  prediction\n",
       "50   everybody chill sony may separate global flags...      1           2\n",
       "56   sony announced new 500 gb playstation 4 bundle...      0           1\n",
       "59   could hit 3rd console war wrote characters wen...      1           2\n",
       "60   sun daily lenovo sony marshall london on-trend...      0           1\n",
       "61   one voted 5th popular song time poll conducted...      0           1\n",
       "62   that's i think may weird last minute deal sony...      0           1\n",
       "66   sony 1st 1 film summer war room fought compton...      0           1\n",
       "68           uncharted march may thing bring back sony      0           1\n",
       "71   ok ... sony announced playable games line toky...      0           1\n",
       "76   i got love sony they disappear tomorrow someon...      2           0\n",
       "78   sony pictures last monday sony pictures releas...      0           1\n",
       "79   sony's project morpheus review a virtual reali...      0           1\n",
       "80   for info would need contact sony apologies inc...      2           1\n",
       "82   tomorrow morning usa vs brazil live sony six h...      0           1\n",
       "83   sony xperia z5 premium news sony may found sol...      0           1\n",
       "84   it deepika tamasha :) sony ... may sony one br...      0           1\n",
       "90                  sony's 20th anniversary sale going      0           1\n",
       "91   ms using strategy sony used last gen spend 1st...      0           1\n",
       "92   ps4 firmware 3.0 as may know sony releasing 3....      0           1\n",
       "95   sony zeiss showdown announcement friday 11 cam...      0           1\n",
       "98      sony's 20th anniversary classic playstation :d      0           1\n",
       "99   apple microsoft 1st pairing tech bastards cine...      1           0\n",
       "108  here's gets wrong star wars toys right force f...      1           0\n",
       "110  um ... i know familiar star wars i shops satur...      1           0\n",
       "120      if married someone like star wars may divorce      1           2\n",
       "121  everyone definitely go watch star wars decembe...      2           0\n",
       "125  did disney make back 3/4 4 billion paid lucasf...      1           0\n",
       "128                  why never old star wars may force      0           2\n",
       "133    in mood watch star wars like 2 i class tomorrow      1           0\n",
       "138  i've problems email received information satur...      2           1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "everybody chill sony may separate global flagship phone works unfortunately\n",
      "sony announced new 500 gb playstation 4 bundle uncharted 4 stored october 9 400 us 450 canada\n",
      "could hit 3rd console war wrote characters went sony's side\n",
      "sun daily lenovo sony marshall london on-trend smartphones some smartphone trends come go ...\n",
      "one voted 5th popular song time poll conducted sony\n",
      "that's i think may weird last minute deal sony mirrors edge solid 12 tv's\n",
      "sony 1st 1 film summer war room fought compton 9.3 m 8.8 m\n",
      "uncharted march may thing bring back sony\n",
      "ok ... sony announced playable games line tokyo games show 2015 september 15 interestingly ...\n",
      "i got love sony they disappear tomorrow someone else would replace try harder consumers\n",
      "sony pictures last monday sony pictures released first theatrical trailer concussion studio's ...\n",
      "sony's project morpheus review a virtual reality headset may change future gaming ...\n",
      "for info would need contact sony apologies inconvenience may caused ai\n",
      "tomorrow morning usa vs brazil live sony six hd 6 am\n",
      "sony xperia z5 premium news sony may found solution overheating snapdragon 810 ...\n",
      "it deepika tamasha :) sony ... may sony one brand partners\n",
      "sony's 20th anniversary sale going\n",
      "ms using strategy sony used last gen spend 1st party nd less marketing deal\n",
      "ps4 firmware 3.0 as may know sony releasing 3.0 update ps4 sometime near\n",
      "sony zeiss showdown announcement friday 11 camcorder otus\n",
      "sony's 20th anniversary classic playstation :d\n",
      "apple microsoft 1st pairing tech bastards cinematic universe stay till credits see eating shawarma sony\n",
      "here's gets wrong star wars toys right force friday via\n",
      "um ... i know familiar star wars i shops saturday everywhere\n",
      "if married someone like star wars may divorce\n",
      "everyone definitely go watch star wars december 18th it'll terrible movie\n",
      "did disney make back 3/4 4 billion paid lucasfilms force friday 3b star wars merchandise sales\n",
      "why never old star wars may force\n",
      "in mood watch star wars like 2 i class tomorrow\n",
      "i've problems email received information saturday's star wars secret cinema can resend\n"
     ]
    }
   ],
   "source": [
    "df_result[df_result['label']!=df_result['prediction']][20:50]\n",
    "\n",
    "df1=df_result[df_result['label']!=df_result['prediction']][20:50]\n",
    "for x in df1['text']:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, for the first sentence, the model predicts it as negative, while the sentence is neutral actually. I think this is because the sentence contains the word 'unfortunately'. For the second sentence, I personally think it should be neutral without looking at the label, but it is actually positive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 What sorts of phenomena do you see and speculate on why you see these errors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that many positive/negative tweets are mis-classified to neutral one. From the result of logistic regression, we can see the `P|(precision of neutral)` is low compared to `P+(precision of positive)`. Since precision is fraction of relevant instances among the retrieved instances, it means many non-neutral tweets are classified as neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 Are there distinct differences between classifiers or are differences difficult to see from your results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+------+------+--------+------+------+------+-------+------+------+------+-------+\n",
      "|  Classifier | F1+  | F1|  | F1-  | F1_avg |  R+  |  R|  |  R-  | R_avg |  P+  |  P|  |  P-  | P_avg |\n",
      "+-------------+------+------+------+--------+------+------+------+-------+------+------+------+-------+\n",
      "|    Vader    | 0.45 | 0.45 | 0.58 |  0.5   | 0.57 | 0.36 | 0.66 |  0.53 | 0.37 | 0.6  | 0.52 |  0.5  |\n",
      "| Naive Bayes | 0.62 | 0.43 | 0.45 |  0.5   | 0.57 | 0.46 | 0.53 |  0.52 | 0.69 | 0.41 | 0.4  |  0.5  |\n",
      "|   Logistic  | 0.65 | 0.48 | 0.45 |  0.53  | 0.6  | 0.51 | 0.49 |  0.54 | 0.7  | 0.45 | 0.42 |  0.53 |\n",
      "+-------------+------+------+------+--------+------+------+------+-------+------+------+------+-------+\n"
     ]
    }
   ],
   "source": [
    "performance(Vader_Score1, Naive_Bayes_Score1, Log_Score1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the result table shows, we can see that there is a great difference between the result of Vader and the results of Naive Bayes, Logistic regression. Vader performs better in $R-$ and $P|$ while other two methods perform better in $R+$ and $P+$. It may because Vader calculates the scores by adding up the score for each word, while the other two methods try to learn the features from the observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5 How important was tokenization / feature extraction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It helps improve accuracy when doing sentiment analysis. For example, excluding URL will decrease the noise in the sentence because it is only a website link and cannot represent any meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6 If you had more time, what might you do differently?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We would use cross validation to choose exact optimal parameters for the model. \n",
    "* The input data contains emoji but our training data does not contain emoji. We can find some data that includes emoji, so we can train the model with emoji.\n",
    "* We can add more words in Vader lexicon by looking at the mis-classification's score and figuring out the reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7 What questions do you now have about your analysis that you didn't have before starting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How punctuation, hashtag, profile influence the result remains unsolved since the result of each experiment seems the same?\n",
    "\n",
    "* Why Naive bayes and logistic regression did not perform as well as we expected?\n",
    "\n",
    "* Is there any other model that has high accuracy for us to do sentiment analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data without Emoji\n",
    "with open('./data/Dev/INPUT.txt', encoding=\"utf-8\") as file:\n",
    "    final=[]\n",
    "    user_id=[]\n",
    "    for line in file:\n",
    "        tweet=line.split('\\t')[2] \n",
    "        tweet=tweet.encode('ascii', 'ignore').decode('ascii')\n",
    "        user_id.append(line.split('\\t')[0])\n",
    "        final.append(tweet[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data using the same procedure\n",
    "final_hash, final_clean, final_punc = process_tweets(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predict_Label(X_train_dtm, Y_train, X_test_dtm, c, n):   \n",
    "    logreg = LogisticRegression(C=c, max_iter =n, class_weight='balanced', solver='newton-cg')\n",
    "    logreg.fit(X_train_dtm, Y_train)\n",
    "    Y_prediction = logreg.predict(X_test_dtm)\n",
    "    \n",
    "    return list(Y_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict label using logistic model\n",
    "model_x = train_clean+dev_clean+test_clean\n",
    "model_y = train_y+ dev_y+test_y\n",
    "x_test = final_clean[:]\n",
    "\n",
    "# Convert to dataframe\n",
    "df_dict = {'text':x_test}\n",
    "df = pd.DataFrame(df_dict)\n",
    "X_test = df['text']\n",
    "\n",
    "X_train, Y_train=convert_label(model_x, model_y)\n",
    "X_train_dtm, X_test_dtm=vectorizer(X_train, X_test)\n",
    "\n",
    "label=Predict_Label(X_train_dtm, Y_train, X_test_dtm, c=0.1, n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert output to dataframe\n",
    "output = {'id': user_id,'label':label}\n",
    "df = pd.DataFrame(output)\n",
    "    \n",
    "# Convert label\n",
    "df['label'] = df['label'].map({0:'positive', 1:'neutral', 2:'negative'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out to tab-separated csv\n",
    "path='./data/Dev/Output.csv'\n",
    "df.to_csv(path, index=False, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
